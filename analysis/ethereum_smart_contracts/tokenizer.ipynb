{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPuIaYYJLjvJ"
   },
   "source": [
    "# Training a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfwnEMBRLjvT"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"/data/forta/ethereum/train.txt\",\n",
    "                                           \"val\": \"/data/forta/ethereum/val.txt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRIVpv8ZLjvT",
    "outputId": "7cf127e1-3fcd-40f1-d82e-9d29d60c02e2"
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T29hShJ1LjvX"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    aux_dataset = dataset[\"train\"]\n",
    "    for start_idx in range(0, len(aux_dataset), 400):\n",
    "        samples = aux_dataset[start_idx : start_idx + 400]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QDOpAR1LjvX"
   },
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# print(old_tokenizer.tokenize(\"PUSH1 PUSH1 MSTORE PUSH1 CALLDATASIZE LT PUSH2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoMI6s6JLjvY"
   },
   "outputs": [],
   "source": [
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 524)\n",
    "# print(old_tokenizer.tokenize(\"PUSH1 PUSH1 MSTORE PUSH1 CALLDATASIZE LT PUSH2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoHgMflNLjvZ"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"/data/forta/ethereum/tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb",
     "timestamp": 1708376608320
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
