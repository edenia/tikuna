{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPuIaYYJLjvJ"
   },
   "source": [
    "# Train a new tokenizer based on smart contract opcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfwnEMBRLjvT"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"/data/forta/ethereum/text/pretraining/pretraining_train.csv\",\n",
    "                                           \"val\": \"/data/forta/ethereum/text/pretraining/pretraining_val.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = -1\n",
    "for row in dataset[\"train\"][\"text\"]:\n",
    "    length_of_the_messages = row.split(\" \")\n",
    "    max = len(length_of_the_messages) if len(length_of_the_messages) > max else max\n",
    "print(\"Max number of words = \", max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T29hShJ1LjvX"
   },
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    batch_size = 400\n",
    "    aux_dataset = dataset[\"train\"]\n",
    "    for start_idx in range(0, len(aux_dataset), batch_size):\n",
    "        samples = aux_dataset[start_idx : start_idx + batch_size]\n",
    "        yield samples[\"text\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QDOpAR1LjvX"
   },
   "outputs": [],
   "source": [
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# print(old_tokenizer.tokenize(\"PUSH1 PUSH1 MSTORE PUSH1 CALLDATASIZE LT PUSH2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoMI6s6JLjvY"
   },
   "outputs": [],
   "source": [
    "vocab_size = 524\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, vocab_size)\n",
    "# print(old_tokenizer.tokenize(\"PUSH1 PUSH1 MSTORE PUSH1 CALLDATASIZE LT PUSH2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoHgMflNLjvZ"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"/data/forta/ethereum/tokenizer\")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/course/en/chapter6/section2.ipynb",
     "timestamp": 1708376608320
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
